# hemoMIPs

The hemoMIPs pipeline is a fast and efficient analysis pipeline for MIP targeted NGS-datasets. It runs highly automated using conda und snakemake and can be set to use GATK v4 or GATK v3 for data processing. It reports benign and likely pathogenic variants in a userfriendly HTML report that shows detailed performance statistics and results for a pooled sequencing cohort for each sample and a general summary for the whole cohort.

## Pre-requirements

### Conda

The pipeline depends on snakemake (that wraps up the scripts and runs them highly automated). We use Conda as environment managment software. Conda can install snakemake itself and the neccessary software with all its dependencies automatically. Conda installation guidlines can be found here:

https://conda.io/docs/user-guide/install/index.html

### Snakemake

Next, you just have to install Snakemake using Conda. This can be done by creating the main environment to run the hemoMIPs environment which contains snakemake using the following command:

```bash
conda env create -n hemoMIPs --file environment.yaml
```

### ensembl-vep

We use Ensembl Variant Effect Predictor to predict variant effects. To install vep run:

```bash
conda env create -n vep-env --file envs/vep-env.yml

conda activate vep-env
```
Adjust the Path to your pipeline location of the following command. This command will download the human VEP cache which is 14G.  \
This can take a while. \
If you already have the VEP database, simply adjust the path to your database in the config.yml. Note that we run the pipeline using VEP v98.


```bash
vep_install -a cf -s homo_sapiens -y GRCh37 -c /~PathTo~/hemoMIPs/vep â€“CONVERT
```



## Config

Almost ready to go:
You need to download the human reference and other files to run the pipeline and adjust the locations of these files in the `config.yml`.\
An example config can be found in `example_config.yml`. \
 \
We are aligning against the 1000 Genomes phase 2 build of the human reference `hs37d5.fa.gz`: \
`wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz` \
`wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.fai` \
You also need the bwa and dictionary index of this file. \
See https://gatkforums.broadinstitute.org/gatk/discussion/2798/howto-prepare-a-reference-for-use-with-bwa-and-gatk\

hemoMIPs uses 1000G annotation of your target region which can be generated with following command using your target_coordinates.bed:
```bash
tabix -h -B ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20110521/ALL.wgs.phase1_release_v3.20101123.snps_indels_sv.sites.vcf.gz <( awk 'BEGIN{OFS="\t"}{print $1,$2-50,$3+50}' target_coords.bed | sort -k1,1 -k2,2n -k3,3n | bedtools merge ) | bgzip -c > phase1_release_v3.20101123.snps_indels_svs.on_target.vcf.gz

tabix -p vcf phase1_release_v3.20101123.snps_indels_svs.on_target.vcf.gz
```

VEP uses following reference genome file: \
`wget ftp://ftp.ensembl.org/pub/release-72/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.72.dna.toplevel.fa.gz` \

Finally: Set the Paths and parameters in your config. You can find a template named "example_config.yml".

## Input
You need your NGS fastq files together with information about your MIP design and the targeted region. An example dataset is present with all crucial files and coresponding datastructures in `input/example_dataset`

Put your NGS fastq files in input/ together with:
- a MIP design file as generated by `https://github.com/shendurelab/MIPGEN` named `hemomips_design.txt`
- a samfile header named `new_header.sam`
- a coresponding barcode sample assignment file named `sample_index.lst`
- the target coordinates of your MIPs named `target_coords.bed`
- the target coordinates of the captured sequences named `targets.intervalls`
- a file containing known benign variants (can be left blank) named `benignVars.txt`


## Run pipeline

Ready to go! If you run the pipeline on a cluster see the `cluster.json` for an estimate of minimum requirements for the individual jobs. Note that this depends on your dataset size so you may have to adjust this.

```bash
conda activate hemoMIPs
# dry run to see if everything works
snakemake  --use-conda --configfile example_config.yml -n
# run the pipeline
snakemake  --use-conda --configfile example_config.yml
```





## Optional

### GATK v3

GATK v4 is included as a conda environment which automatically installs GATK v4.0.4.0 and all its dependencies.
If you prefer to run the pipeline using GATK v3 you need to install both GATK3 versions, if you trust GATK4, this step is not needed:

GATK 3.2.2 (https://software.broadinstitute.org/gatk/download/auth?package=GATK-archive&version=3.2-2-gec30cee)
GATK 3.4-46 (https://software.broadinstitute.org/gatk/download/auth?package=GATK-archive&version=3.4-46-gbc02625)


### Shed Skin

Shed Skin is an experimental compiler, that can translate pure, but implicitly statically typed Python (2.4-2.6) programs into optimized C++. To fasten the read overlapping process one of our python scripts have to be translated to C++ with Shed Skin.
This will speed up the analysis but is not crucial for the implementation.
First we need an environment with python v2.6 and the requirements for Shed Skin. Therefore we created the environment file `envs/shedskin.yml`. Be sure that you are in your root hemoMIPs pipeline folder.

```bash
# create a new environment
conda env create -f envs/shedskin.yml -n shedskin

mkdir -p ~/miniconda3/envs/shedskin/etc/conda/activate.d
mkdir -p ~/miniconda3/envs/shedskin/etc/conda/deactivate.d

echo '#!/bin/sh
export LD_LIBRARY_PATH="$HOME/miniconda3/envs/shedskin/lib:$LD_LIBRARY_PATH"' > ~/miniconda3/envs/shedskin/etc/conda/activate.d/env_vars.sh

echo '#!/bin/sh
unset LD_LIBRARY_PATH' > ~/miniconda3/envs/shedskin/etc/conda/deactivate.d/env_vars.sh

conda activate shedskin
```
Then download and install Shed Skin v0.9.4 into the bin directory of the hemoMIPs pipeline.

```bash
# Download Shed Skin 0.9.4
wget https://github.com/shedskin/shedskin/releases/download/v0.9.4/shedskin-0.9.4.tgz
# Create bin folder
mkdir -p bin
# Extract Shed Skin and remove file
tar -xzf shedskin-0.9.4.tgz -C bin
rm shedskin-0.9.4.tgz
# Install Shed Skin
cd bin/shedskin-0.9.4
python setup.py install
```
Now we can test the shed Skin installation. We have to modify the `Makefile` to point to the GC library.
```bash
shedskin -L ~/miniconda3/envs/shedskin/include test
sed -i '3s|$| -L ~/miniconda3/envs/shedskin/lib|' Makefile
make
./test
```
The result should look like
```
*** SHED SKIN Python-to-C++ Compiler 0.9.4 ***
Copyright 2005-2011 Mark Dufour; License GNU GPL version 3 (See LICENSE)

[analyzing types..]
********************************100%
[generating c++ code..]
[elapsed time: 1.29 seconds]

hello, world!
```
If the installation or test fails please have a look a the [Shed Skin Dokumentation](https://shedskin.readthedocs.io/en/latest/).

#### Compiling MergeTrimReads.py script

Now we need to compile the `MergeTrimReads.py` script using Shed Skin:

```bash
# Go to the script folder
cd ../../scripts/pipeline2.0
# create Makefile and edit it
shedskin -e -L ~/miniconda3/envs/shedskin/include MergeTrimReads
sed -i '3s|$| -L ~/miniconda3/envs/shedskin/lib|' Makefile
# Compile!
make
cd ../../
```

